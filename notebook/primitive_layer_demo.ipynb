{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cff567",
   "metadata": {},
   "source": [
    "1. Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59374e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!nvidia-smi\n",
    "import torch\n",
    "\n",
    "# Set device immediately\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498fa61",
   "metadata": {},
   "source": [
    "2. Clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672cb39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/cocooda/JEPAPrimitiveLayer.git\n",
    "%cd JEPAPrimitiveLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b71183",
   "metadata": {},
   "source": [
    "3. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b39dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the cloned repo to Python path\n",
    "sys.path.append(\"/kaggle/working/JEPAPrimitiveLayer\")\n",
    "\n",
    "EMBED_DIM = 128\n",
    "PATCH_SIZE = 4\n",
    "IMAGE_H = 32\n",
    "IMAGE_W = 32\n",
    "TOKEN_DIM = 3 * PATCH_SIZE * PATCH_SIZE\n",
    "ACTION_DIM = 4\n",
    "MASK_RATIO = 0.15\n",
    "VICREG_WEIGHT = 0.1\n",
    "DRIFT_WEIGHT = 0.05\n",
    "JEPA_WEIGHT = 1.0\n",
    "EMA_DECAY = 0.99\n",
    "BATCH_SIZE = 8\n",
    "NUM_STEPS = 50\n",
    "LR = 1e-3\n",
    "\n",
    "DATA_ROOT = \"/kaggle/input/test1t/exported_maps\"\n",
    "CKPT_DIR = \"/kaggle/working/checkpoints\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "CKPT_PATH = os.path.join(CKPT_DIR, \"primitive_layer.pth\")\n",
    "\n",
    "from utils.dataset import DrivingSceneDataset\n",
    "from utils.patch_utils import unpatchify\n",
    "from models.primitive_layer import PrimitiveLayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a432377",
   "metadata": {},
   "source": [
    "4. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46aeb1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DrivingSceneDataset(DATA_ROOT)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "print(f\"Loaded {len(dataset)} samples from {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ebf39",
   "metadata": {},
   "source": [
    "5. Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00c582",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = PrimitiveLayer(patch_size=PATCH_SIZE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc2d9a",
   "metadata": {},
   "source": [
    "6. Check if checkpoints exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f50f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"/kaggle/working/checkpoints\", exist_ok=True)\n",
    "ckpt_path = \"/kaggle/working/checkpoints/primitive_layer.pth\"\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.eval()\n",
    "    print(f\"Checkpoint loaded from {ckpt_path}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Training from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea821351",
   "metadata": {},
   "source": [
    "7. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab1596",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for step, (frames, kin) in enumerate(loader):\n",
    "    if step >= NUM_STEPS:\n",
    "        break\n",
    "    frames, kin = frames.to(DEVICE), kin.to(DEVICE)\n",
    "    optimizer.zero_grad()\n",
    "    _, total_loss, loss_dict = model(frames, kin)\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(total_loss.item())\n",
    "    print(f\"Step {step+1}/{NUM_STEPS} | Loss = {total_loss.item():.6f} | JEPA={loss_dict['JEPA'].item():.6f} VICReg={loss_dict['VICReg'].item():.6f} Drift={loss_dict['Drift'].item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a7c30",
   "metadata": {},
   "source": [
    "8. Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a35c1a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), ckpt_path)\n",
    "print(f\"Checkpoint saved at {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b2528",
   "metadata": {},
   "source": [
    "9. Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2be6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss on Tokenized BEV Dataset\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a28deb9",
   "metadata": {},
   "source": [
    "10. Inference & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25943b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample_frames, sample_kin = next(iter(loader))\n",
    "sample_frames, sample_kin = sample_frames.to(DEVICE), sample_kin.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_tokens, _, _ = model(sample_frames, sample_kin)  # (B, N, D)\n",
    "\n",
    "# Unpatchify tokens to images (B, D, H, W)\n",
    "B, N, D = pred_tokens.shape\n",
    "ph = pw = int(N ** 0.5)\n",
    "pred_imgs = unpatchify(pred_tokens.cpu(), ph, pw, patch_size=PATCH_SIZE)  # (B, D, H, W)\n",
    "\n",
    "# --- Convert to 3-channel RGB for plotting ---\n",
    "if pred_imgs.shape[1] >= 3:\n",
    "    pred_imgs_rgb = pred_imgs[:, :3, :, :]  # take first 3 channels\n",
    "else:\n",
    "    # if less than 3 channels, repeat or average\n",
    "    pred_imgs_rgb = pred_imgs.repeat(1, 3 // pred_imgs.shape[1], 1, 1)\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(12,4))\n",
    "for i in range(min(4, B)):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.imshow(pred_imgs_rgb[i].permute(1,2,0))  # (H, W, 3)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
